{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Auto-scaling with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Yuhang Lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import keras\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are 2019, 03, 27, 20, 08, 56, 19.08310159958135\n",
      "At time 2019, 03, 27, 20, 08, 58: 71.24085340259741 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 00: 59.997746961038956 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 02: 42.27412045918367 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 04: 24.92243770573566 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 06: 26.968270459183675 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 08: 29.994252745591936 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 10: 29.714518686868686 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 12: 22.86435292620865 percent of CPU was used.\n",
      "At time 2019, 03, 27, 20, 09, 14: 22.1947184 percent of CPU was used.\n",
      "Processed 5085 lines.\n"
     ]
    }
   ],
   "source": [
    "cpu_list = []\n",
    "time_list = []\n",
    "window = 50\n",
    "max_threshold = 70\n",
    "min_threshold = 40\n",
    "with open('cdata.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            if line_count < 10:\n",
    "                print(f'At time {row[0]}: {row[1]} percent of CPU was used.')\n",
    "            line_count += 1\n",
    "            time_list.append(row[0])\n",
    "            cpu_list.append(float(row[1]))\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_action = []\n",
    "index = 0\n",
    "while index + 1 < len(cpu_list):\n",
    "    if cpu_list[index + 1] > max_threshold:\n",
    "        correct_action.append(1)\n",
    "    elif cpu_list[index + 1] < min_threshold:\n",
    "        correct_action.append(0)\n",
    "    else:\n",
    "        correct_action.append(2)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                1200      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 12)                288       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 6)                 72        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,566\n",
      "Trainable params: 1,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple fully connected layer model \n",
    "# with 200 hidden units in first layer\n",
    "# and 1 sigmoid output\n",
    "inputs = keras.layers.Input(shape=(1, window))\n",
    "flattened_layer = keras.layers.Flatten()(inputs)\n",
    "full_connect_1 = keras.layers.Dense(units=24,activation='relu',use_bias=False,)(flattened_layer)\n",
    "full_connect_2 = keras.layers.Dense(units=12,activation='relu',use_bias=False,)(full_connect_1)\n",
    "full_connect_3 = keras.layers.Dense(units=6,activation='relu',use_bias=False,)(full_connect_2)\n",
    "sigmoid_output = keras.layers.Dense(1,activation='sigmoid',use_bias=False)(full_connect_3)\n",
    "policy_network_model = keras.models.Model(inputs=inputs,outputs=sigmoid_output)\n",
    "policy_network_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = keras.layers.Input(shape=(1,),name='episode_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_loss(episode_reward):\n",
    "    def loss(y_true,y_pred):\n",
    "        # feed in y_true as actual action taken \n",
    "        # if actual action was up, we feed 1 as y_true and otherwise 0\n",
    "        # y_pred is the network output(probablity of taking up action)\n",
    "        # note that we dont feed y_pred to network. keras computes it\n",
    "        \n",
    "        # first we clip y_pred between some values because log(0) and log(1) are undefined\n",
    "        tmp_pred = keras.layers.Lambda(lambda x: keras.backend.clip(x,0.05,0.95))(y_pred)\n",
    "        # we calculate log of probablity. y_pred is the probablity of taking up action\n",
    "        # note that y_true is 1 when we actually chose up, and 0 when we chose down\n",
    "        # this is probably similar to cross enthropy formula in keras, but here we write it manually to multiply it by the reward value\n",
    "        tmp_loss = keras.layers.Lambda(lambda x:-y_true*keras.backend.log(x)-(1-y_true)*(keras.backend.log(1-x)))(tmp_pred)\n",
    "        # multiply log of policy by reward\n",
    "        policy_loss=keras.layers.Multiply()([tmp_loss,episode_reward])\n",
    "        return policy_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = keras.layers.Input(shape=(1,),name='episode_reward')\n",
    "policy_network_train = keras.models.Model(inputs=[inputs,episode_reward],outputs=sigmoid_output)\n",
    "\n",
    "my_optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
    "policy_network_train.compile(optimizer=my_optimizer,loss=m_loss(episode_reward),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reward Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(action, current_index):\n",
    "    '''\n",
    "    Reward function for the policy.\n",
    "    '''\n",
    "    if cpu_list[current_index] > max_threshold and action == 0:\n",
    "        return -1\n",
    "    if cpu_list[current_index] < min_threshold and action == 1:\n",
    "        return -1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy_network):\n",
    "    states_list = [] # shape = (x,80,80)\n",
    "    up_or_down_action_list=[] # 1 if we chose up. 0 if down\n",
    "    rewards_list=[]\n",
    "    network_output_list=[]\n",
    "    policy_output_list = []\n",
    "    start = 0\n",
    "    while start + window < len(cpu_list):\n",
    "        network_input = np.array(cpu_list[start:start + window]) / 100\n",
    "        processed_network_input = np.expand_dims(network_input,axis=0) \n",
    "        states_list.append(processed_network_input)\n",
    "        reshaped_input = np.expand_dims(processed_network_input,axis=0) \n",
    "        up_probability = policy_network.predict(reshaped_input,batch_size=1)[0][0]\n",
    "        network_output_list.append(up_probability)\n",
    "        policy_output_list.append(up_probability)\n",
    "        actual_action = 3 - np.random.choice(a=[2,3],size=1,p=[up_probability,1-up_probability]) # 2 is up. 3 is down\n",
    "        action_value = actual_action[0]\n",
    "        up_or_down_action_list.append(action_value)\n",
    "        reward = get_reward(action_value, start)\n",
    "        rewards_list.append(reward)\n",
    "        start += 1\n",
    "    return states_list,up_or_down_action_list,rewards_list,network_output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function is plain and simple, nothing to discuss about it.\n",
    "so lets play 1 game and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list,up_or_down_action_list,rewards_list,network_output_list = generate_episode(policy_network_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of states= 5034\n",
      "shape of each state=(1, 50)\n",
      "length of rewards= 5034\n"
     ]
    }
   ],
   "source": [
    "print(\"length of states= \"+str(len(states_list)))# this is the number of frames\n",
    "print(\"shape of each state=\"+str(states_list[0].shape))\n",
    "print(\"length of rewards= \"+str(len(rewards_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5181897, 0.51763225, 0.51690525, 0.51989734, 0.51774114, 0.5203116, 0.51980734, 0.52035016, 0.524815, 0.5246905, 0.52539843, 0.52092016, 0.51977086, 0.5289869, 0.52500904, 0.5269466, 0.5277174, 0.52554977, 0.5267595, 0.53151715]\n"
     ]
    }
   ],
   "source": [
    "# lets see sample of policy output\n",
    "print(network_output_list[30:50]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the network is not trained, its output is about 50% all time. meaning . that it does not know which action is better now and outputs a probablity of about 0.5 for all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets see a sample what we actually did: 1 means we went up, 0 means down\n",
    "up_or_down_action_list[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "# lets see sample of rewards\n",
    "print(rewards_list[50:100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count win=2710\n",
      "count lose=2324\n",
      "count zero rewards=0\n"
     ]
    }
   ],
   "source": [
    "# lets see how many times we won through whole game:\n",
    "print(\"count win=\"+str(len(list(filter(lambda r: r>0,rewards_list)))))\n",
    "print(\"count lose=\"+str(len(list(filter(lambda r: r<0,rewards_list)))))\n",
    "print(\"count zero rewards=\"+str(len(list(filter(lambda r: r==0,rewards_list)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example of simluation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate an episode:\n",
    "states_list,up_or_down_action_list,rewards_list,network_output_list = generate_episode(policy_network_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of states= 5034\n",
      "shape of each state=(1, 50)\n",
      "length of rewards= 5034\n"
     ]
    }
   ],
   "source": [
    "print(\"length of states= \"+str(len(states_list)))# this is the number of frames\n",
    "print(\"shape of each state=\"+str(states_list[0].shape))\n",
    "print(\"length of rewards= \"+str(len(rewards_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_reward.shape = (5034, 1)\n",
      "x.shape = (5034, 1, 50)\n",
      "y_true.shape = (5034, 1)\n"
     ]
    }
   ],
   "source": [
    "#preprocess inputs for training: \n",
    "    \n",
    "x=np.array(states_list)\n",
    "\n",
    "#episode_reward=np.expand_dims(process_rewards(rewards_list),1)\n",
    "episode_reward=np.expand_dims(rewards_list,1)\n",
    "y_tmp = np.array(up_or_down_action_list) # 1 if we chose up, 0 if down\n",
    "y_true = np.expand_dims(y_tmp,1) # modify shape. this is neccassary for keras\n",
    "\n",
    "\n",
    "print(\"episode_reward.shape =\",episode_reward.shape)\n",
    "print(\"x.shape =\",x.shape)\n",
    "print(\"y_true.shape =\",y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5034/5034 [==============================] - 0s 69us/step - loss: -0.1141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff798a13898>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model with inputs and outputs.\n",
    "policy_network_train.fit(x=[x,episode_reward],y=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a helper function to create a batch of simulations\n",
    "# and after the batch simulations, preprocess data and fit the network\n",
    "def generate_episode_batches_and_train_network(n_batches=10):\n",
    "    batch_state_list=[]\n",
    "    batch_up_or_down_action_list=[]\n",
    "    batch_rewards_list=[]\n",
    "    batch_network_output_list=[]\n",
    "    for i in range(n_batches):\n",
    "        states_list,up_or_down_action_list,rewards_list,network_output_list = generate_episode(policy_network_model)   \n",
    "        batch_state_list.extend(states_list[15:])\n",
    "        batch_network_output_list.extend(network_output_list[15:])\n",
    "        batch_up_or_down_action_list.extend(up_or_down_action_list[15:])\n",
    "        batch_rewards_list.extend(rewards_list[15:])\n",
    "    \n",
    "    episode_reward=np.expand_dims(batch_rewards_list,1)\n",
    "    x=np.array(batch_state_list)\n",
    "    y_tmp = np.array(batch_up_or_down_action_list)\n",
    "    y_true = np.expand_dims(y_tmp,1)\n",
    "    policy_network_train.fit(x=[x,episode_reward],y=y_true)\n",
    "\n",
    "    return batch_state_list,batch_up_or_down_action_list,batch_rewards_list,batch_network_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: -0.1097\n",
      "i=0\n",
      "count win=30675\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: -0.2729\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0404\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0411\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0412\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0418\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0418\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0409\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0422\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0416\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0421\n",
      "i=10\n",
      "count win=46260\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0418\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0420\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0415\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0421\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0417\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0417\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0420\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0417\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0419\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0418\n",
      "i=20\n",
      "count win=46255\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0418\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0413\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0406\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0415\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0417\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0418\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0416\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0415\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0420\n",
      "Epoch 1/1\n",
      "50190/50190 [==============================] - 1s 25us/step - loss: 0.0417\n",
      "i=30\n",
      "count win=46253\n",
      "Training is completed\n"
     ]
    }
   ],
   "source": [
    "train_n_times = 31\n",
    "for i in range(train_n_times):\n",
    "    states_list,up_or_down_action_list,rewards_list,network_output_list=generate_episode_batches_and_train_network(10)\n",
    "    if i%10==0:\n",
    "        print(\"i=\"+str(i))\n",
    "        rr=np.array(rewards_list)\n",
    "        print('count win='+str(len(rr[rr>0]))) \n",
    "        policy_network_model.save(\"policy_network_model_nn.h5\")\n",
    "        policy_network_model.save(\"policy_network_model_nn\"+str(i)+\".h5\")\n",
    "        with open('rews_model_simple.txt','a') as f_rew:\n",
    "            f_rew.write(\"i=\"+str(i)+'       reward= '+str(len(rr[rr > 0])))\n",
    "            f_rew.write(\"\\n\")\n",
    "print(\"Training is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Loading model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                1200      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 12)                288       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 6)                 72        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,566\n",
      "Trainable params: 1,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "policy_network_model=keras.models.load_model(\"policy_network_model_nn30.h5\")\n",
    "policy_network_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(expected, actual, start):\n",
    "    counter = 0\n",
    "    for i in range(start, len(actual)):\n",
    "        if expected[i] == 2 or expected[i] == actual[i]:\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836251557479179\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "for i in range(3):\n",
    "    states_list,up_or_down_action_list,rewards_list,network_output_list = generate_episode(policy_network_model)\n",
    "    res = evaluate(correct_action, up_or_down_action_list, window)\n",
    "    accuracy_list.append(res / len(correct_action))\n",
    "print(sum(accuracy_list) / len(accuracy_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
